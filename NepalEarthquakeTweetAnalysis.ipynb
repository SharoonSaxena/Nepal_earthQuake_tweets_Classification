{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NepalEarthquakeTweetAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/SharoonSaxena/Nepal_earthQuake_tweets_Classification/blob/master/NepalEarthquakeTweetAnalysis.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "J_ZJQ9FqcQK0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Nepal Earthquake Tweet Classification"
      ]
    },
    {
      "metadata": {
        "id": "26B8-b6icQK0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## About the dataset\n",
        "\n",
        "#### This data set consists $18233$ number of Tweets extracted from the Social Media Website  Twitter.com, as these tweets were extracted duting the Nepal Eathquake crisis. This dataset contains tweets addressing the Nepal Eathquake. The Tweets are labeleed as follow:\n",
        "### $(1)$ Irrelevant tweets, they have no significance.\n",
        "### $(2)$ Tweets that specify a need or demand for help or supplies.\n",
        "### $(3)$ Tweets that Offer Help and Supplies."
      ]
    },
    {
      "metadata": {
        "id": "oBacGuYKcQK1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem\n",
        "#### Any good Smaritan would have found it to be vey difficult to detect a tweet needing help or the ones ready to offer help.\n",
        "\n",
        "#### Our job as a Beginner Data Scientist is to detect and classify the relevent tweets."
      ]
    },
    {
      "metadata": {
        "id": "AV8GkThhcQK1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing the Libraries"
      ]
    },
    {
      "metadata": {
        "id": "jdC2dI1scQK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd              # For data Manipulation\n",
        "import numpy as np               # For efficient numerical computation\n",
        "import seaborn as sns            # For \"pretty\" Visualisations\n",
        "import matplotlib.pyplot as plt  # Library containing Visualisation Tools\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud  # For WordCloud Visualisation\n",
        "import datetime                  # Allows us to use functions related to date and time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRRgnXz-cQK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing the Dataset"
      ]
    },
    {
      "metadata": {
        "id": "j-hMjFGZmWBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: 18SuBxMWHO8zId13GMycCywOda53FWL-W\n",
        "file_id = '18SuBxMWHO8zId13GMycCywOda53FWL-W'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WzaWEK68meo-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))import io\n",
        "df = pd.read_csv(io.StringIO(downloaded.GetContentString()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cy-STAY9cQMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Nepal_Earthquake')\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQEBzDsccQK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## First look at the dataset"
      ]
    },
    {
      "metadata": {
        "id": "zuUETipPcQK-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head(7)                    #Prints first 7 rows from the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHejErnZcQLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.tail(7)                   #Prints last 7 rows of the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vfdoi06BcQLF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.shape    # prints the sahpe of the Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V2uktixRcQLI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.describe() # tells about the basic statistics of the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IPEonKOKcQLK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " There appears to be **no missing data**"
      ]
    },
    {
      "metadata": {
        "id": "5vUPxobCcQLM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['TweetClass'].value_counts()    #Counts the occourance of each catagory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9RNSv2VDcQLO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data appears to be** very Skewed and unbalanced**, the Graph below verifies it."
      ]
    },
    {
      "metadata": {
        "id": "DNDlHAUhcQLP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.countplot(x='TweetClass', data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJfPGSq-cQLS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.duplicated()  # Checking for duplicacy."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GF18qUCacQLV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dupli=df.duplicated()  \n",
        "dupli.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKwLpvOUcQLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All the duplicacy queries are False, i.e. **there are no duplicates.**"
      ]
    },
    {
      "metadata": {
        "id": "Av8CoVq9cQLa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WordCloud\n",
        "\n",
        "#### Data visualizations (like charts, graphs, infographics, and more) give businesses a valuable way to communicate important information at a glance, but what if your raw data is text-based? If you want a stunning visualization format to highlight important textual data points, using a word cloud can make dull data sizzle and immediately convey crucial information.\n",
        "\n",
        "#### Word clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of textual data (such as a speech, blog post, or database), the bigger and bolder it appears in the word cloud."
      ]
    },
    {
      "metadata": {
        "id": "DHKTu6LyrSpt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Segmenting Each Tweet class dor WordCloud Visualisation"
      ]
    },
    {
      "metadata": {
        "id": "JXTgLy2BsoAP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Visualising the most frequent words in the Non-Relevant Tweets."
      ]
    },
    {
      "metadata": {
        "id": "Ws7QQ6wPrzYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer                                               # Vectorises the text\n",
        "\n",
        "# NON RELEVENT\n",
        "non_relevant_df     = df[df['TweetClass'] == 0]                                                           # Filtering out non-relevent Tweets from the dataset\n",
        "non_relevant_vector = CountVectorizer(stop_words = 'english')                                             # Removing: stop_words = Trivial words like a,the,is... etc.\n",
        "non_relevant_dtm    = non_relevant_vector.fit_transform(non_relevant_df['TweetText'].values.astype('U'))  # Implementing CountVectoriser Object\n",
        "non_relevant_words  = non_relevant_vector.get_feature_names()                                             # A Vector of non_relevant words\n",
        "non_relevant_freqs  = non_relevant_dtm.sum(axis = 0).A1                                                   # Storing Frequency of each word\n",
        "non_relevant_result = dict(zip(non_relevant_words,non_relevant_freqs))                                    # Dictionary of word with their frequencies\n",
        "\n",
        "\n",
        "# Visualising\n",
        "wordcloud = WordCloud(background_color = 'white',\n",
        "                   width = 1400,\n",
        "                   height = 800).generate_from_frequencies(non_relevant_result)\n",
        "fig=plt.figure(figsize=(20,20))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.title(\"Most Used words in Non-relevent Tweets\")\n",
        "fig.savefig(\"non_Relevant_wordcloud.png\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mMHwDVcwNVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " There, We can now easily see the most frequently used words in the Non_Relevant tweets."
      ]
    },
    {
      "metadata": {
        "id": "aiTP3QHRwcHu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Visualising the most frequent words in the **Needful Tweets**."
      ]
    },
    {
      "metadata": {
        "id": "jX3tHvMrcQLh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NEED\n",
        "need_df     = df[df['TweetClass'] == 1]                                           # Filtering out NEED Tweets from the dataset\n",
        "need_vector = CountVectorizer(stop_words = 'english')                             # Removing: stop_words = Trivial words like a,the,is... etc.\n",
        "need_dtm    = need_vector.fit_transform(need_df['TweetText'].values.astype('U'))  # Implementing CountVectoriser Object\n",
        "need_words  = need_vector.get_feature_names()                                     # A Vector of non_relevant words\n",
        "need_freqs  = need_dtm.sum(axis = 0).A1                                           # Storing Frequency of each word\n",
        "need_result = dict(zip(need_words,need_freqs))                                    # Dictionary of word with their frequencies\n",
        "\n",
        "\n",
        "# Visualising\n",
        "wordcloud = WordCloud(background_color = 'white',\n",
        "                   width = 1400,\n",
        "                   height = 800).generate_from_frequencies(need_result)\n",
        "fig=plt.figure(figsize=(20,20))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.title(\"Most Used words in NEEDFUL Tweets\")\n",
        "fig.savefig(\"NEED_wordcloud.png\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cq2olvJAx9Va",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " We can Observe that frequent words are associated to what people might have **Needed**"
      ]
    },
    {
      "metadata": {
        "id": "QX70q5BOzV13",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Visualising the most frequent words in the Offer Tweets."
      ]
    },
    {
      "metadata": {
        "id": "6unPye0C0h5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Offer\n",
        "offer_df     = df[df['TweetClass'] == 2]                                              # Filtering out NEED Tweets from the dataset\n",
        "offer_vector = CountVectorizer(stop_words = 'english')                                # Removing: stop_words = Trivial words like a,the,is... etc.\n",
        "offer_dtm    = offer_vector.fit_transform(offer_df['TweetText'].values.astype('U'))   # Implementing CountVectoriser Object\n",
        "offer_words  = offer_vector.get_feature_names()                                       # A Vector of non_relevant words\n",
        "offer_freqs  = offer_dtm.sum(axis = 0).A1                                             # Storing Frequency of each word\n",
        "offer_result = dict(zip(offer_words,offer_freqs))                                     # Dictionary of word with their frequencies\n",
        "\n",
        "\n",
        "# Visualising\n",
        "wordcloud = WordCloud(background_color = 'white',\n",
        "                   width = 1400,\n",
        "                   height = 800).generate_from_frequencies(offer_result)\n",
        "fig=plt.figure(figsize=(20,20))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.title(\"Most Used words in OFFER Tweets\")\n",
        "fig.savefig(\"OFFER_wordcloud.png\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2O9pC5C21_l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, one can easily observe the words used in offering various resources to the victims."
      ]
    },
    {
      "metadata": {
        "id": "lE0_q2lX8CUH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing A Classification Model"
      ]
    },
    {
      "metadata": {
        "id": "nU6bR0Vn8OQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting the Dataset"
      ]
    },
    {
      "metadata": {
        "id": "bMS0IaJWAzsR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = df.TweetText         # Independent Variable\n",
        "y = df.TweetClass      # Dependent Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sks5HbUMqv__",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.125, random_state=42)     # Splitting the 12.5% data to test set\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KSpgpzD-Wat",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using Tf-Idf Vectoriser.\n",
        "\n",
        "Normally, A simple CountVectoriser gives equal emphasis to every word, relevant or not.\n",
        "\n",
        "Whereas, Term Frequesncy (Tf) - Inverse Document Frequesncy (Idf Vectoriser give more emphasis to words)\n",
        "\n",
        "This gives emphasis to the words that Appeas Less often and hold more value over others.\n"
      ]
    },
    {
      "metadata": {
        "id": "I-qBTNpXs5sq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preparing Vocabulory from the Training set and also Vectorising the Training set.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectoriser = TfidfVectorizer( stop_words = ['english'],ngram_range = [1,2])     # ngrams allow us to retain sequential pattern of words\n",
        "x_train_vectorised = vectoriser.fit_transform(x_train.values.astype('U'))\n",
        "print(x_train_vectorised.shape)\n",
        "\n",
        "# Vectorising the test set\n",
        "x_test_vectorised = vectoriser.transform(x_test)\n",
        "print(x_test_vectorised.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "umHEKYBwFdMZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Making a learning model\n",
        "For this Text Classification Example Multinomial Naive Baye's works best for data containing wordcounts.\n",
        "\n",
        "### GridSearcCV\n",
        "In machine learning, two tasks are commonly done at the same time in data pipelines: cross validation and (hyper)parameter tuning. Cross validation is the process of training learners using one set of data and testing it using a different set. Parameter tuning is the process to selecting the values for a model’s parameters that maximize the accuracy of the model."
      ]
    },
    {
      "metadata": {
        "id": "7XT8b_pewp4R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# making skeletal model\n",
        "classifier = MultinomialNB()                         #Base Estimator\n",
        "\n",
        "# Set of parameters we want to try for out Model\n",
        "parameters = { 'alpha' : [0.13,0.15,0.17]}\n",
        "\n",
        "#Running the Model with above chosen parameter\n",
        "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters , scoring = 'accuracy', cv = 5, n_jobs = -1 , verbose = 2)\n",
        "grid_scores = grid_search.fit(x_train_vectorised , y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VlrGp9mVL9Tb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets find out the **Best parameters** found by the GridSearchCV"
      ]
    },
    {
      "metadata": {
        "id": "9Zx_itxEIG_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print( grid_search.best_score_)\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "79dUZPwnNaL6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "now we use the best paramets obtained to make our final model."
      ]
    },
    {
      "metadata": {
        "id": "pEDiUijkMHJc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making the Final Classification model.\n",
        "classifier = MultinomialNB( alpha = 0.15)\n",
        "tick =datetime.datetime.now()\n",
        "classifier.fit(x_train_vectorised, y_train)\n",
        "tock=datetime.datetime.now()\n",
        "lr_train_time = tock - tick\n",
        "print(\"Time taken for training MultinimialNB model is model = \" + str(lr_train_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9rBwWZ4OcS8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "now we predict the classes of tweets on the test set"
      ]
    },
    {
      "metadata": {
        "id": "xN-_uqFlN5ty",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tick=datetime.datetime.now()\n",
        "result = classifier.predict(x_test_vectorised)\n",
        "tock=datetime.datetime.now()\n",
        "lr_pred_train_time = tock - tick\n",
        "print('Time taken to predict the data points in the Test set is : ' + str(lr_pred_train_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDymDB4UPy3x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analysing Model Performance"
      ]
    },
    {
      "metadata": {
        "id": "24m7GIOXPETl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Accuracy on the Training set"
      ]
    },
    {
      "metadata": {
        "id": "KFvV56SoOs3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "k = classifier.score(x_train_vectorised, y_train)\n",
        "print('the Accuracy on the Training set comes out to be : ' + str(k))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M75Cp86kPTuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Accuracy on the test Set"
      ]
    },
    {
      "metadata": {
        "id": "obK7LGwzPQgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "k = classifier.score(x_test_vectorised, y_test)\n",
        "print('the Accuracy on the Training set comes out to be : ' + str(k))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pEN1mHIJPp6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Lets print the classification report on test set"
      ]
    },
    {
      "metadata": {
        "id": "-44wOsmsW0jg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as skm\n",
        "print(skm.classification_report( y_test ,result  ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDPBnLKIXfDz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OuSjbtieOZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The f1 score comes out to be 0.95 which ensures that Model implemented is good."
      ]
    },
    {
      "metadata": {
        "id": "WgTGNB7aexI5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Although We can still observe that Overall Recall of the Need ansd Offer tweets are fairly low."
      ]
    }
  ]
}